---
GeekDocCollapseSection: true
weight: 2
---
# How the back end works _v 2.0
By Sam Mazarei
All previous versions have been deprecated[.](I waited my whole life to type that) 

It begins in the front end...    

When a student submission is generated, two files are uploaded as objects  to the submitter S3 bucket.   One is a single `.py` file provided by the student(s), and the other a `.json` file generated by the front end logic containing all the requisite data  to properly process the submission.   
This json file will contain the following:

* submissionid  : a 128 bit MD5 Hash unique to the specific students, admin, event,  and problem.
* admin         : The admin's unique identifier (e.g "m.soltys").  
* event         : A string identifier that is unique to the admin.
* problem       : A string identifier unique to the event.
* tokens        : A(n) list/array of tokens generated by the front end upon event registration.  

This example would be named  `/admin/event/assignment/<subid>/<subid>.json` , where the `subid` 
is calculated at the front-end during submission but can be found here as [submission_example.json](/Back-End/ExampleSubmissionJSON/)



```
{
  "subid": "fc55c0190dde2bc413d8d1e79fb8cca2",
  "admin": "m.soltys",
  "event": "aws_labs",
  "assignment": "lab5_containers",
  "tokens": [
    "d823640ab3b0f7a4a2bc9fc89661e940",
    "240669d4326dea48bba75e066b90b76f",
    "4b31d568d86a9350d746c7c2fe9bf5c8"
  ]
}        
```
S3 buckets only store objects, not files, but a pseudo file system can be achieved by using full paths as keys according to a pseudo file system hierarchy. Using values from the above example, this file and the student submission would be uploaded as:  

```
/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py
```

  and   

```
/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json
```

Furthermore, at the time of event/assignment creation by an admin, data relevant to the assignment such as the test and grade case in/out files are also uploaded to the S3 bucket. By the time the example submission from above is submitted, test and grade case files exist in the bucket as:  

```
/m.soltys/aws_labs/lab5_containers/grading/test.in
```

```
/m.soltys/aws_labs/lab5_containers/grading/test.out
```

```
/m.soltys/aws_labs/lab5_containers/grading/grade.in
```

```
/m.soltys/aws_labs/lab5_containers/grading/grade.out
```

**_Every assignment in every event will contain a pseudo directory called grading. Again, this directory does not exist but is part of the naming convention. Test and grade case in and out files will be found in the grading pseudo directory._**   
These files are provided by the admin, and are formatted such that each line is an input, and for each input the student submission is executed with the input passed in as a command line argument. Ten test case inputs would result in both `.in` and `.out` files that are ten lines in size. `.out` files are the expected output when the respective line of input is passed to the student submission.   

All of these files/objects will be used by the backend to process student submissions.

## A json upload to the S3 bucket triggers a lambda function... (Not just new object creation!)

A json upload to the S3 bucket triggers a lambda function written in python. This lambda function generates a message whose body contains the key, value pair `subdata: <key of object triggering lambda>` and places it on the SQS queue. This message is a json object maded up of many other key, value pairs.  This is an example of a [message body]()  for example above:  

```
{
  "subdata" : "/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json"
}        
```
...once this message arrives to the queue...

## The back end makes its inglorious entrance...  

The back end's _processStudentsTestCases.py_  constantly polls the SQS queue for a message and when one is returned a client thread is spawned with the message passed in to its constructor. Then the main thread calls  `start()`  on this client thread, which in turn calls the client thread's `run()` method, then begins polling the SQS queue for the next message.      

### Client Thread  

When the client thread's constructor executes, it populates all instance variables except the student email list. This is done later in the process. The entry point for all client thread's is  `run()`, and when the thread executes it begins by 

* Creating a directory using the subid from the subdata filename, for example  
  * `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/`  
  * It is important to check if this directory already exists, implying a previous usbmission is still being graded. In this case the thread quietly exits and the current submission is processed at a later time.
* Changing present working directories into the newly cerated directory, from there retrieving both the newly submitted 
  json and python files from the S3 bucket, for example:  
  * `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json`  
  * `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py`
    
### Query database for test case parameters
From the data in the json file, our script will then   
* query the database for the test inputs and outputs, and generate files for comparison, for example `test.in` and `test.out`.   

These files will be formatted so that individual test case input for the specific problem is placed on a new line in `test.in`, 
and the appropriate output expected for each input is placed on a new line in `test.out`.   
The directory structure at this point:  

* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json`  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py`  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.in`  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.out`  

### Execute submitted code in Python 3.9 container  
At this point we have the code to execute, the inputs to run, and the appropriate outputs for comparison. We are using a
python 3 docker image with a custom, executable script [executeSubmission.py](https://github.com/comp350spring2021/SubmitterProject/blob/SamadMazarei/back-end/executables/executeSubmission.py) installed that will execute the submitted 
code against all provided inputs from test.in, generating and generating the output file. Our script will now:
* Create a new thread that will in turn execute the appropriate `docker run` command.  
  * `docker run -it --rm --name="subidfc55c0190dde2bc413d8d1e79fb8cca2" -v "$PWD":/usr/src/submitter -w /usr/src/submitter 
    python:3.9 executeSubmission.py fc55c0190dde2bc413d8d1e79fb8cca2`  
    

This will spin up our container and keep it alive until it finishes executing. The present working directory gets bind-mounted
to a newly created directory in the container `/usr/src/submitter`. Read more on the container and how it works [here](./docker/Docker.md).
Since the default for bind mount is read-write, this allows our script in the container to generate an output that can be read 
by our main thread. It is important that our script's  
* Main thread __MUST__ wait for the worker thread to finish executing before moving on.  


### Return output to the front-end via submitter S3 bucket and clean house  

_At the time of this writing it is understood that the way to return the output generated by the user's submission is for the backend to
upload the newly generated `fc55c0190dde2bc413d8d1e79fb8cca2.out` to the submitter S3 bucket for the front end to retrieve._   

Now that our worker thread has finished executing, our script's current directory should contain:  
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.in`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.out`
* `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.out`

Our script will now:  
* upload `fc55c0190dde2bc413d8d1e79fb8cca2.out` to submitter S3 bucket.  
* change working directories up one to `/home/ec2-user/`.  
* delete the directory `/home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/`

Finally, now that the submission has been processed, the working space has been cleaned, and the output generated has been
sent to the next destination, our script must:
* delete the message that was originally retrieved from the submitter SQS queue.  

It is important to remember that when our script initially retrieved the message from the SQS queue, that it doers not _remove_ the 
message from the queue, it only hides it from any other process being able to retrieve it for the ammount of time defined in 
the parameters for the SQS instance.  